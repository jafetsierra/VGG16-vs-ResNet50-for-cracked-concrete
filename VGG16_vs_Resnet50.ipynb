{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JrehPHcJtI60"
      },
      "source": [
        "<a id=\"item41\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# VGG16 vs Resnet50 for cracked concrete images classification \n",
        "\n",
        "*  Jafet Israel sierra lagos\n",
        "*  jafet.sierra.l@gmail.com"
      ],
      "metadata": {
        "id": "VXEAne-8eSPS"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s55VowlptI60"
      },
      "source": [
        "## Download Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OQYz9WmftI61",
        "outputId": "1fe3c079-8187-40c9-ac97-da578de15f4c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting wget\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9675 sha256=9118e2da35c8a027402e7c8581d3b185daab25d093f090d47f5179592b119f84\n",
            "  Stored in directory: /root/.cache/pip/wheels/a1/b6/7c/0e63e34eb06634181c63adacca38b79ff8f35c37e3c13e3c02\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n"
          ]
        }
      ],
      "source": [
        "! pip install wget"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q6eBMSqFtI62",
        "outputId": "7b9a69f0-dc92-4665-d0fb-baecc9a9ed3f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-06-12 23:14:43--  https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/concrete_data_week4.zip\n",
            "Resolving s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)... 67.228.254.196\n",
            "Connecting to s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)|67.228.254.196|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 261483817 (249M) [application/zip]\n",
            "Saving to: ‘concrete_data_week4.zip’\n",
            "\n",
            "concrete_data_week4 100%[===================>] 249.37M  27.2MB/s    in 8.7s    \n",
            "\n",
            "2022-06-12 23:14:52 (28.7 MB/s) - ‘concrete_data_week4.zip’ saved [261483817/261483817]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget 'https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/concrete_data_week4.zip'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Je0C9v2GtI62"
      },
      "source": [
        "  "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/concrete_data_week4.zip"
      ],
      "metadata": {
        "id": "n_sxe0qSukPd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "6OB2pBnPtI63"
      },
      "source": [
        "<a id=\"item42\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69xD-RhXtI63"
      },
      "source": [
        "## VGG16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "MKo1PJJQtI6_"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\n",
        "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten, Input\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Preprocessing\n",
        "vgg_image_generator = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
        "#Train loader\n",
        "vgg_train_loader = vgg_image_generator.flow_from_directory(\n",
        "    '/content/concrete_data_week4/train',\n",
        "    batch_size=100,\n",
        "    target_size=(224,224),\n",
        "    shuffle=True\n",
        ")\n",
        "#Valid loader\n",
        "vgg_valid_loader = vgg_image_generator.flow_from_directory(\n",
        "    '/content/concrete_data_week4/valid',\n",
        "    batch_size=100,\n",
        "    target_size=(224,224),\n",
        "    shuffle=True\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QwGhWEMo5LEc",
        "outputId": "2c3d9847-3db3-403b-a216-1a39904bb60d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 30001 images belonging to 2 classes.\n",
            "Found 9501 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "scrolled": true,
        "id": "_QWwC1gxtI7A"
      },
      "outputs": [],
      "source": [
        "def load_pretrained_model():\n",
        "  input_t = Input(shape=(224,224,3))\n",
        "  model = VGG16(weights='imagenet',input_tensor=input_t,include_top=False)\n",
        "  model.trainable = False\n",
        "  return model\n",
        "\n",
        "base_model = load_pretrained_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "2-Q07Fr-tI7A"
      },
      "outputs": [],
      "source": [
        "def add_head(feature_extractor_model):\n",
        "  prev_output  = feature_extractor_model.output\n",
        "  head_flatten = Flatten()(prev_output)\n",
        "  head_dense1  = Dense(64,activation='relu',kernel_regularizer='l2')(head_flatten)\n",
        "  head_drop    = Dropout(0.5)(head_dense1)\n",
        "  head_dense2  = Dense(2,activation='softmax')(head_drop) \n",
        "\n",
        "  model = Model(inputs=feature_extractor_model.input,outputs=head_dense2)\n",
        "\n",
        "  opt = Adam(1e-4)\n",
        "  model.compile(\n",
        "      optimizer=opt,\n",
        "      loss='categorical_crossentropy',\n",
        "      metrics=['accuracy']\n",
        "  )\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "XhFxIIRNtI7A"
      },
      "outputs": [],
      "source": [
        "model = add_head(base_model)\n",
        "#model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "JxxUQ6u0tI7A"
      },
      "outputs": [],
      "source": [
        "def scheduler(epoch,lr):\n",
        "  if epoch<1:\n",
        "    return lr\n",
        "  else:\n",
        "    return lr*tf.math.exp(-0.105)\n",
        "batch = 100\n",
        "def train_model_gen(model,train_gen,val_gen,epochs):\n",
        "  early_stop = EarlyStopping(patience=1,monitor='accuracy')\n",
        "\n",
        "  \n",
        "  return model.fit(\n",
        "      train_gen,\n",
        "      steps_per_epoch  = train_gen.samples // batch,\n",
        "      validation_data  = val_gen,\n",
        "      validation_steps = val_gen.samples // batch,\n",
        "      epochs=epochs,\n",
        "      callbacks=[tf.keras.callbacks.LearningRateScheduler(scheduler),early_stop],\n",
        "      verbose=1\n",
        "      )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A-2UgVqbtI7B",
        "outputId": "124445b9-702b-4c5f-d7de-5242fae677d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "300/300 [==============================] - 148s 445ms/step - loss: 0.6524 - accuracy: 0.9879 - val_loss: 0.3897 - val_accuracy: 0.9962 - lr: 1.0000e-04\n",
            "Epoch 2/3\n",
            "300/300 [==============================] - 117s 388ms/step - loss: 0.3103 - accuracy: 0.9981 - val_loss: 0.2470 - val_accuracy: 0.9980 - lr: 9.0032e-05\n",
            "Epoch 3/3\n",
            "300/300 [==============================] - 116s 385ms/step - loss: 0.2061 - accuracy: 0.9988 - val_loss: 0.1746 - val_accuracy: 0.9971 - lr: 8.1058e-05\n"
          ]
        }
      ],
      "source": [
        "history = train_model_gen(model,vgg_train_loader,vgg_valid_loader,3)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history.history"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gc6oWN8EALJY",
        "outputId": "b797fe2a-51e6-4210-bcec-c7a8110f13e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'accuracy': [0.9841811060905457, 0.997491717338562],\n",
              " 'loss': [0.6830583214759827, 0.32709500193595886],\n",
              " 'lr': [1e-04, 9.003245e-05],\n",
              " 'val_accuracy': [0.9964210391044617, 0.996842086315155],\n",
              " 'val_loss': [0.4073544144630432, 0.2653428912162781]}"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aszUDzrvtI7B"
      },
      "source": [
        "   "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def plots(history,model_name:str):\n",
        "  plt.figure(figsize=(20,10))\n",
        "  plt.subplot(121)\n",
        "  try:\n",
        "    plt.plot(history.history['accuracy'])\n",
        "    plt.plot(history.history['val_accuracy'])\n",
        "  except KeyError:\n",
        "    plt.plot(history.history['acc'])\n",
        "    plt.plot(history.history['val_acc'])\n",
        "  plt.title(f'Accuracy vs epochs {model_name}')\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.ylabel('Accuracy')\n",
        "  plt.legend(['training', 'validation'])\n",
        "  plt.subplot(122)\n",
        "  plt.plot(history.history['loss'])\n",
        "  plt.plot(history.history['val_loss'])\n",
        "  plt.xlabel('epochs')\n",
        "  plt.ylabel('loss')\n",
        "  plt.legend(['training_loss','val_loss'])\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "C3HmX4A-_2vo"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plots(history,'vgg16')"
      ],
      "metadata": {
        "id": "6UxIgdPLiNEe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Is6cL_zctI7B"
      },
      "source": [
        "<a id=\"item43\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhd9boV2tI7B"
      },
      "source": [
        "## ResNet50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "2531YhB0tI7C"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.applications.resnet50 import ResNet50\n",
        "from tensorflow.keras.applications.resnet50 import preprocess_input as resnet_preprocess"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "LTb_b05MtI7C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3dbdd3eb-2d23-464d-ca54-eaecf8f772b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 30001 images belonging to 2 classes.\n",
            "Found 9501 images belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "#Preprocessing\n",
        "resnet_image_generator = ImageDataGenerator(preprocessing_function=resnet_preprocess)\n",
        "#Train loader\n",
        "resnet_train_loader = resnet_image_generator.flow_from_directory(\n",
        "    '/content/concrete_data_week4/train',\n",
        "    batch_size=100,\n",
        "    target_size=(224,224),\n",
        "    shuffle=True\n",
        ")\n",
        "#Valid loader\n",
        "resnet_valid_loader = resnet_image_generator.flow_from_directory(\n",
        "    '/content/concrete_data_week4/valid',\n",
        "    batch_size=100,\n",
        "    target_size=(224,224),\n",
        "    shuffle=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "TYHQqHmwtI7C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "892a103b-f18f-48ad-cbb8-43ced974104e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "94773248/94765736 [==============================] - 1s 0us/step\n",
            "94781440/94765736 [==============================] - 1s 0us/step\n"
          ]
        }
      ],
      "source": [
        "def load_resnet_pretrained_model():\n",
        "  input_t = Input(shape=(224,224,3))\n",
        "  model = ResNet50(weights='imagenet',input_tensor=input_t,include_top=False)\n",
        "  model.trainable = False\n",
        "  return model\n",
        "\n",
        "resnet_base_model = load_resnet_pretrained_model()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "resnet_model = add_head(resnet_base_model)\n",
        "#resnet_model.summary()"
      ],
      "metadata": {
        "id": "b8nq78eddpHM"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resnet_history = train_model_gen(resnet_model,resnet_train_loader,resnet_valid_loader,2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SClVXXDSdy6a",
        "outputId": "9c3586cc-482f-4bb7-dde9-084df97eb287"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "300/300 [==============================] - 114s 368ms/step - loss: 0.4449 - accuracy: 0.9948 - val_loss: 0.2043 - val_accuracy: 0.9980 - lr: 1.0000e-04\n",
            "Epoch 2/2\n",
            "300/300 [==============================] - 109s 362ms/step - loss: 0.1512 - accuracy: 0.9980 - val_loss: 0.1167 - val_accuracy: 0.9975 - lr: 9.0032e-05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plots(resnet_history,'resnet50')"
      ],
      "metadata": {
        "id": "cp7XGtgJhySA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resnet_test_loader = resnet_image_generator.flow_from_directory(\n",
        "    '/content/concrete_data_week4/test',\n",
        "    batch_size=100,\n",
        "    target_size=(224,224),\n",
        "    shuffle=False\n",
        ")\n",
        "vgg_test_loader = vgg_image_generator.flow_from_directory(\n",
        "    '/content/concrete_data_week4/test',\n",
        "    batch_size=100,\n",
        "    target_size=(224,224),\n",
        "    shuffle=False\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iOYF_3yWTf9E",
        "outputId": "dcf06730-1fd3-458c-a0e8-58a3f7656512"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 500 images belonging to 2 classes.\n",
            "Found 500 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## comparison"
      ],
      "metadata": {
        "id": "oeu-R8pbeoqw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#vgg16\n",
        "vgg_loss, vgg_acc = model.evaluate(\n",
        "    vgg_test_loader,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0hfDCJ_iWFcb",
        "outputId": "6ac5f6f1-2ed7-4370-dddc-59cbe1af2564"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5/5 [==============================] - 2s 289ms/step - loss: 0.1673 - accuracy: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Resnet50\n",
        "resnet_los, resnet_acc = resnet_model.evaluate(\n",
        "    resnet_test_loader\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "miQinQdCWE1U",
        "outputId": "f11f502e-b3da-4e80-aa6d-50dd5ab46ddb"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5/5 [==============================] - 2s 272ms/step - loss: 0.1087 - accuracy: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Results of both models on test data\\n\")\n",
        "print('*ResNet50-- loss: {:.2f}, accuracy: {}'.format(resnet_los,resnet_acc))\n",
        "print('*VGG16----- loss: {:.2f}, accuracy: {}'.format(vgg_loss,vgg_acc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PHOIF8f0YXzE",
        "outputId": "a6f53e79-901c-46b1-ca30-4da4c9f549b6"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results of both models on test data\n",
            "\n",
            "*ResNet50-- loss: 0.11, accuracy: 1.0\n",
            "*VGG16----- loss: 0.17, accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yjDzUqTbtI7D"
      },
      "source": [
        "Use the following cells to make your predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "QQfmOKAStI7D"
      },
      "outputs": [],
      "source": [
        "resnet_pred = resnet_model.predict(\n",
        "    resnet_test_loader,\n",
        "    steps=1\n",
        ")\n",
        "\n",
        "vgg_pred = model.predict(\n",
        "    vgg_test_loader,\n",
        "    steps=1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "gnYX0OAqtI7E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8f26cac-479f-4b2b-f404-5b616e8ef10f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The results for the first 5 elements on test data with resNet50 model are:  ['Negative', 'Negative', 'Negative', 'Negative', 'Negative']\n"
          ]
        }
      ],
      "source": [
        "print('The results for the first 5 elements on test data with resNet50 model are: ',\n",
        "['Negative' if np.argmax(x)==0 else 'Positive' for x in resnet_pred[0:5,:]]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "MvFJDeCxtI7E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8efa4e8-ef8b-47e7-db51-bbe8781ff422"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The results for the first 5 elements on test data with VGG16 model are:  ['Negative', 'Negative', 'Negative', 'Negative', 'Negative']\n"
          ]
        }
      ],
      "source": [
        "print('The results for the first 5 elements on test data with VGG16 model are: ',\n",
        "['Negative' if np.argmax(x)==0 else 'Positive' for x in vgg_pred[0:5,:]]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3lsAkitWtI7E"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "name": "VGG16_vs_ResNet50.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}